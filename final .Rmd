---
title: "Final Project"
author: "Paul Hendriksen"
date: "5/2/2019"
output: 
  html_document:
    theme: darkly
---

#CLIMATE CHANGE
***

![](/Users/paulhendriksen/climate1.jpg){width=100%}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Intro
In this walkthrough we will be dealing with the very popular question of "is climate change real?". 

##Table of contents

First of all we need to import all the libraries we will need to perform our experiment.
```{r load_data, message=FALSE}
library(tidyverse)
library(plotly)
library(rvest)
library(grid)
library(gridExtra)
```

###Creating our hypothesis
Starting off with our project we will discuss the notion of hypothesis testing!!. First we start off with the null hypothesis which is usually the opposite of what you are trying to claim. For example our null hypothesis which is usually denoted by //H_n//
###Ataining the data
Fist we will go over the most basic, and honestly one of the most important parts of Data Science. Which is actually gathering the data and getting it into a tidy form that is easy to work with. 

-First we find some data that seems like it will support our experiment. For this part I would highly reccomend using chrome as your browser as it makes for a much easier time. 
-Once you found the site with the data go to the top right corner of the screen and click on the three vertical dots. Then scroll down to More Tools, and go to developer tools. After you have done all that you should see at the right side of the screen some HTMl code. This is where we can find the css selector so we can extract the data to R.
<center>![](/Users/paulhendriksen/Downloads/pic.png)</center>

```{r}
url <- "https://www.worldometers.info/world-population/world-population-by-year/"

dl_tab <- url %>%
  read_html() %>%
  html_node(".table-condensed") %>%
  html_table()

colnames(dl_tab)[2] <- "Population"
(dl_tab)
```

-Ok so now that we have our table all set up it is important to note the


Lets try and see if there is a linear relationship between year and 
```{r}
population <- dl_tab %>% 
  ggplot(aes(x=Year, y=Population)) + 
  geom_point()

population + ggtitle("Population Vs. Year")
```
As one can see this plot is very ugly since we have so many data points. Now what we can do is remove some of the data points since we are only looking for the relationship between year and population growth. 
First we will remove all the years before 1958 since this is when population growth really started to grow, 
```{r}
revised_population <- dl_tab %>% 
  select(1, 2) %>% 
  filter(Year >= 1958)

revised_population_plot <- revised_population %>% 
  sample_n(15) %>% 
  ggplot(aes(x=Year, y=Population)) + 
    geom_point() 

revised_population_plot + ggtitle("Population Vs. Year")
```
Now to illustrate the real power of poperly cleaning data we will display the two graphs side by side.
```{r}
grid.arrange(population, revised_population_plot, nrow = 1)
```
```{r}
regres <- lm(Population~Year, revised_population)
broom::tidy(regres)
```

Now that we have all the imformation for population we will scrap some data from the NOAA for levels of carbon dioxide in the air.

Scraping this data is a little different from the way we got the data on population. Since this data is not already set up for us in an html table we have to work a little harder to tidy this up, but not too much. 

Step one: Download the data to your local machine. 
Step two: Import the data into Excel. We do this because we want to get it into CSV form so it is easily readable in R. 
Step three: Now save the CSV file back onto your local machine.
Step four: Now we can use the same method to read the csv file in R

```{r}
myData <- read_csv(file="/Users/paulhendriksen/Downloads/Book3.csv")
colnames(myData)[1] <- "Year"
colnames(myData)[5] <- "carbonLevels"
newData <- select(myData, 1, 5)

head(newData)
```

Now that we have a pretty good setup of carbon levels we can plot them vs year to see the how well they match up with our population increase. 

Also since there are multiple data points for each year lets take the average of each year. 
This is where we take advantage of the function group_by() which will group our dataset by a requirement.

```{r}
newData <- newData %>% 
  group_by(Year) %>% 
  summarize(meanCarbon = mean(carbonLevels))

plot <- newData %>% 
  ggplot(aes(x = Year, y = meanCarbon)) +
    geom_point() + 
    geom_smooth(method = lm)

plot
```

Another very important concept in data science is being able to take two data frames and combine them on a certain attribute. We will use the concept of the merge command which is essentially the same concept as the join command in SQL. 
-Because we only want data that is in both of the data frames we will use the merge function such that it will only have
```{r}
merged_df <- merge(x = dl_tab, y = newData, by = "Year") %>% 
  select(1, 2, 8) %>% 
  type_convert()

head(merged_df)

popVsCarbon <- merged_df %>% 
  sample_n(30) %>% 
  ggplot(aes(x=Population, meanCarbon)) +
  geom_point() + 
  geom_smooth(method = lm)

popVsCarbon + theme(axis.text.x = element_text(angle=90))
```

Now we can use the idea of a linear regression to check the null hypothesis of no relationship between population growth and carbon dioxide levels. 
We need to determine if this set is a good fit for a linear regression. 
```{r}
regresion <- lm(meanCarbon~Population, data = merged_df)
summary(regresion)
```

Now that we have showed that there is a very strong correlation between population growth and increased carbon emisions. Now let us use our new data science tools to try and understand some of the negative effects that come with the increased carbon use. 
So what would be the main effects of increased carbon consumption? First, and foremost would be the increased global temparature. This might not sound that bad to people, but having the planet heat up a few degrees is catastrophic. 
We will start by showing the number of natural disasters over the past 100 years If you felt comfortable with the way we scrapped the data from the two other datasets than i highly recommend you try this one out yourself before you look at the given code. The data is on the website https://ourworldindata.org/natural-disasters and is already in csv format for you 

```{r}
natural_disasters <- read_csv(file = "/Users/paulhendriksen/Downloads/number-of-natural-disaster-events.csv")
colnames(natural_disasters)[4] <- "NumberOfDisasters"
natural_disasters <- natural_disasters[, -2]
head(natural_disasters)

```
Ok excellent we got the information on the number of natural disasters over the years so lets see if it follows a similiar trend to that of the population and carbon emissions. 
```{r}

natural_disasters_plot <- natural_disasters  %>% 
  group_by(Entity) %>% 
  ggplot(aes(x=Year, y=NumberOfDisasters, color = Entity)) +
  geom_point() + 
  geom_smooth(method =lm)

natural_disasters_plot
  
```
Ok so this graph is not bad as it shows us the distribution, but using one simple method called facet_grid we can break these into individual plots 
```{r}
natural_disasters_plot <- natural_disasters  %>% 
  filter(Entity != "Impact") %>% 
  group_by(Entity) %>% 
  ggplot(aes(x=Year, y=NumberOfDisasters, color = Entity)) +
  facet_wrap(~Entity, nrow = 2) +
  geom_point() + 
  geom_smooth(method =lm)

natural_disasters_plot + theme(axis.text.x = element_text(angle=90))
```
Thus using this we can clearly see that natural disasters are rising just as scientist said they would. Obviously floods, and extreme weather are experiencing the greatest spike. However, as data scientist we need to be able to evaluate the data and determine if there is a trend.  
